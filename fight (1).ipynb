{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be6df4c-bde4-4b7a-8a2a-aeaf41e8c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded. Expected input: (None, 16, 64, 64, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "üß† Result: ‚ö†Ô∏è Fight Detected!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ViolenceDetectorTFLite:\n",
    "    def __init__(self, model_path, fight_threshold=0.85):\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        self.input_shape = self.input_details[0]['shape']\n",
    "        self.frame_sequence = deque(maxlen=16)\n",
    "        self.prediction_history = deque(maxlen=8)\n",
    "        self.fight_threshold = fight_threshold\n",
    "\n",
    "    def preprocess(self, frames):\n",
    "        resized = [cv2.resize(f, (64, 64)) for f in frames]\n",
    "        normalized = np.array(resized) / 255.0\n",
    "        return normalized.astype(np.float32).reshape(1, 16, 64, 64, 3)\n",
    "\n",
    "    def predict(self, frame):\n",
    "        self.frame_sequence.append(frame)\n",
    "        if len(self.frame_sequence) < 16:\n",
    "            return \"Safe\"\n",
    "\n",
    "        input_data = self.preprocess(list(self.frame_sequence))\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "        output_score = self.interpreter.get_tensor(self.output_details[0]['index'])[0][0]\n",
    "\n",
    "        self.prediction_history.append(output_score)\n",
    "        avg_score = sum(self.prediction_history) / len(self.prediction_history)\n",
    "\n",
    "        if avg_score > self.fight_threshold:\n",
    "            return \"üö® FIGHT DETECTED - TAKE ACTION\"\n",
    "        else:\n",
    "            return \"‚úÖ Safe\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_path = \"violence_model.tflite\"\n",
    "    detector = ViolenceDetectorTFLite(model_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Error: Webcam not accessible!\")\n",
    "        return\n",
    "\n",
    "    print(\"‚úÖ Webcam opened. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ö†Ô∏è Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        small_frame = cv2.resize(frame, (320, 240))\n",
    "\n",
    "        # Predict violence label\n",
    "        status = detector.predict(small_frame)\n",
    "        color = (0, 0, 255) if \"FIGHT\" in status else (0, 255, 0)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        overlay_text = f\"{status} | {timestamp}\"\n",
    "\n",
    "        cv2.putText(frame, overlay_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), color, 3)\n",
    "\n",
    "        cv2.imshow(\"üì∑ Real-Time Violence Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4684893-c28e-4b26-b1da-2577860c77fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:46:43.350 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Kalindi Mishra\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model once\n",
    "@st.cache_resource\n",
    "def load_violence_model():\n",
    "    model = load_model(\"violence_detection_model (1).h5\")\n",
    "    return model\n",
    "\n",
    "model = load_violence_model()\n",
    "\n",
    "# Frame extraction function\n",
    "def extract_frames_from_video(video_path, num_frames=16, target_size=(64, 64)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames < num_frames:\n",
    "        return None\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n",
    "    frames = []\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i in frame_indices:\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            frame = frame / 255.0\n",
    "            frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Prediction function\n",
    "def predict_violence(frames, model):\n",
    "    if frames.shape != (16, 64, 64, 3):\n",
    "        return \"‚ùå Frame input shape mismatch\"\n",
    "    input_data = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(input_data)[0][0]\n",
    "    return \"‚ö†Ô∏è Fight Detected!\" if prediction > 0.5 else \"‚úÖ No Fight Detected.\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.set_page_config(page_title=\"Violence Detection\", layout=\"centered\")\n",
    "st.title(\"üîç Violence Detection in Videos\")\n",
    "\n",
    "video_file = st.file_uploader(\"Upload a Video\", type=[\"mp4\", \"avi\", \"mov\"])\n",
    "\n",
    "if video_file:\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as temp_video:\n",
    "        temp_video.write(video_file.read())\n",
    "        video_path = temp_video.name\n",
    "\n",
    "    st.video(video_file)\n",
    "    st.write(\"‚è≥ Extracting frames and running prediction...\")\n",
    "\n",
    "    frames = extract_frames_from_video(video_path)\n",
    "\n",
    "    if frames is None or len(frames) < 16:\n",
    "        st.error(\"‚ùå Couldn't extract enough frames from the video.\")\n",
    "    else:\n",
    "        result = predict_violence(frames, model)\n",
    "        st.success(f\"üß† Prediction Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939d937-53d4-424e-a5b1-2aed72d97c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
