{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a60197-7042-42b6-90d3-4ec347e1c6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Violence: 100%|████████████████████████████████████████████████████████████| 1000/1000 [04:41<00:00,  3.55it/s]\n",
      "Loading NonViolence: 100%|█████████████████████████████████████████████████████████| 1000/1000 [02:32<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 254ms/step - accuracy: 0.5353 - loss: 0.6960 - val_accuracy: 0.7801 - val_loss: 0.4619\n",
      "Epoch 2/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 251ms/step - accuracy: 0.7652 - loss: 0.4853 - val_accuracy: 0.8491 - val_loss: 0.3669\n",
      "Epoch 3/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 280ms/step - accuracy: 0.8175 - loss: 0.3844 - val_accuracy: 0.8645 - val_loss: 0.3070\n",
      "Epoch 4/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 287ms/step - accuracy: 0.8441 - loss: 0.3330 - val_accuracy: 0.8798 - val_loss: 0.2781\n",
      "Epoch 5/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 299ms/step - accuracy: 0.8754 - loss: 0.3020 - val_accuracy: 0.8849 - val_loss: 0.2758\n",
      "Epoch 6/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 296ms/step - accuracy: 0.8876 - loss: 0.2602 - val_accuracy: 0.9028 - val_loss: 0.2524\n",
      "Epoch 7/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 308ms/step - accuracy: 0.9068 - loss: 0.2419 - val_accuracy: 0.8951 - val_loss: 0.2728\n",
      "Epoch 8/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 299ms/step - accuracy: 0.9258 - loss: 0.1935 - val_accuracy: 0.9105 - val_loss: 0.2544\n",
      "Epoch 9/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 251ms/step - accuracy: 0.9284 - loss: 0.1774 - val_accuracy: 0.9028 - val_loss: 0.2807\n",
      "Epoch 10/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 249ms/step - accuracy: 0.9425 - loss: 0.1315 - val_accuracy: 0.8824 - val_loss: 0.4755\n",
      "Epoch 11/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 268ms/step - accuracy: 0.9545 - loss: 0.1149 - val_accuracy: 0.9028 - val_loss: 0.2878\n",
      "Epoch 12/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 260ms/step - accuracy: 0.9690 - loss: 0.0740 - val_accuracy: 0.9258 - val_loss: 0.2736\n",
      "Epoch 13/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 257ms/step - accuracy: 0.9903 - loss: 0.0415 - val_accuracy: 0.9054 - val_loss: 0.3914\n",
      "Epoch 14/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 253ms/step - accuracy: 0.9808 - loss: 0.0605 - val_accuracy: 0.8951 - val_loss: 0.3301\n",
      "Epoch 15/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 253ms/step - accuracy: 0.9836 - loss: 0.0499 - val_accuracy: 0.8977 - val_loss: 0.4056\n",
      "Epoch 16/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 260ms/step - accuracy: 0.9727 - loss: 0.0910 - val_accuracy: 0.8900 - val_loss: 0.3472\n",
      "Epoch 17/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 267ms/step - accuracy: 0.9870 - loss: 0.0304 - val_accuracy: 0.8875 - val_loss: 0.4494\n",
      "Epoch 18/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 265ms/step - accuracy: 0.9878 - loss: 0.0444 - val_accuracy: 0.8977 - val_loss: 0.4667\n",
      "Epoch 19/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 252ms/step - accuracy: 0.9802 - loss: 0.0600 - val_accuracy: 0.8875 - val_loss: 0.4549\n",
      "Epoch 20/20\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 251ms/step - accuracy: 0.9874 - loss: 0.0567 - val_accuracy: 0.8977 - val_loss: 0.4292\n",
      "Evaluating model...\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 154ms/step - accuracy: 0.8752 - loss: 0.5399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8977\n",
      "Model saved as 'violence_detection_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (64, 64)  # Resize frames to 64x64\n",
    "FRAME_COUNT = 16  # Number of frames per video clip\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "\n",
    "# Function to preprocess a single video\n",
    "def preprocess_video(video_path, img_size=IMG_SIZE, frame_count=FRAME_COUNT):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate frame step to uniformly sample frame_count frames\n",
    "    frame_step = max(1, total_frames // frame_count)\n",
    "    \n",
    "    for i in range(0, total_frames, frame_step):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize and normalize frame\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0  # Normalize to [0,1]\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # If fewer frames, pad with zeros\n",
    "    while len(frames) < frame_count:\n",
    "        frames.append(np.zeros((img_size[0], img_size[1], 3)))\n",
    "    \n",
    "    return np.array(frames[:frame_count])\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(data_dir, classes=['Violence', 'NonViolence']):\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for video_file in tqdm(os.listdir(class_dir), desc=f\"Loading {class_name}\"):\n",
    "            if video_file.endswith('.mp4'):\n",
    "                video_path = os.path.join(class_dir, video_file)\n",
    "                frames = preprocess_video(video_path)\n",
    "                X.append(frames)\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define 3D CNN model\n",
    "def create_model(input_shape=(FRAME_COUNT, IMG_SIZE[0], IMG_SIZE[1], 3)):\n",
    "    model = Sequential([\n",
    "        Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Set dataset path\n",
    "    data_dir = r'C:\\Users\\Namo\\fight recognization\\extract\\Real Life Violence Dataset'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading dataset...\")\n",
    "    X, y = load_dataset(data_dir)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model()\n",
    "    print(\"Training model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save('violence_detection_model.h5')\n",
    "    print(\"Model saved as 'violence_detection_model.h5'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa294ca8-5e12-466d-afb0-b39328986641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset with augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Violence:   1%|▊                                                             | 13/1000 [00:19<24:32,  1.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 440\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete! Check the generated plots and metrics file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 440\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 353\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    350\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNamo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfight recognization\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mReal Life Violence Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset with augmentation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 353\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_dataset(data_dir, augment_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data loaded. Please check your dataset path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 191\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(data_dir, classes, augment_training)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment_training \u001b[38;5;129;01mand\u001b[39;00m (label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(video_files) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# Violence class or small dataset\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):  \u001b[38;5;66;03m# Create 2 augmented versions\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m         aug_frames \u001b[38;5;241m=\u001b[39m preprocess_video(video_path, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m aug_frames \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m             X\u001b[38;5;241m.\u001b[39mappend(aug_frames)\n",
      "Cell \u001b[1;32mIn[1], line 133\u001b[0m, in \u001b[0;36mpreprocess_video\u001b[1;34m(video_path, img_size, frame_count, augment)\u001b[0m\n\u001b[0;32m    130\u001b[0m frame_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_frames \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m frame_count)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_frames, frame_step):\n\u001b[1;32m--> 133\u001b[0m     cap\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_FRAMES, i)\n\u001b[0;32m    134\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (64, 64)\n",
    "FRAME_COUNT = 16\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class VideoAugmentation:\n",
    "    \"\"\"Class for video data augmentation techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_horizontal_flip(frames, prob=0.5):\n",
    "        \"\"\"Randomly flip video horizontally\"\"\"\n",
    "        if random.random() < prob:\n",
    "            return np.flip(frames, axis=2)  # Flip along width axis\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_brightness_adjustment(frames, prob=0.5, factor_range=(0.7, 1.3)):\n",
    "        \"\"\"Randomly adjust brightness\"\"\"\n",
    "        if random.random() < prob:\n",
    "            factor = random.uniform(*factor_range)\n",
    "            frames = frames * factor\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_contrast_adjustment(frames, prob=0.5, factor_range=(0.8, 1.2)):\n",
    "        \"\"\"Randomly adjust contrast\"\"\"\n",
    "        if random.random() < prob:\n",
    "            factor = random.uniform(*factor_range)\n",
    "            mean = np.mean(frames)\n",
    "            frames = (frames - mean) * factor + mean\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_rotation(frames, prob=0.3, angle_range=(-10, 10)):\n",
    "        \"\"\"Randomly rotate frames\"\"\"\n",
    "        if random.random() < prob:\n",
    "            angle = random.uniform(*angle_range)\n",
    "            h, w = frames.shape[1:3]\n",
    "            center = (w // 2, h // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            \n",
    "            rotated_frames = []\n",
    "            for frame in frames:\n",
    "                rotated_frame = cv2.warpAffine(frame, rotation_matrix, (w, h))\n",
    "                rotated_frames.append(rotated_frame)\n",
    "            return np.array(rotated_frames)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_temporal_crop(frames, prob=0.3):\n",
    "        \"\"\"Randomly crop temporal sequence\"\"\"\n",
    "        if random.random() < prob and len(frames) > FRAME_COUNT // 2:\n",
    "            start_idx = random.randint(0, len(frames) - FRAME_COUNT // 2)\n",
    "            end_idx = start_idx + FRAME_COUNT // 2\n",
    "            cropped_frames = frames[start_idx:end_idx]\n",
    "            # Duplicate frames to maintain sequence length\n",
    "            while len(cropped_frames) < FRAME_COUNT:\n",
    "                cropped_frames = np.concatenate([cropped_frames, cropped_frames])\n",
    "            return cropped_frames[:FRAME_COUNT]\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_frame_dropout(frames, prob=0.2, dropout_rate=0.1):\n",
    "        \"\"\"Randomly drop frames and duplicate others\"\"\"\n",
    "        if random.random() < prob:\n",
    "            n_frames = len(frames)\n",
    "            n_drop = int(n_frames * dropout_rate)\n",
    "            drop_indices = random.sample(range(n_frames), n_drop)\n",
    "            \n",
    "            # Replace dropped frames with adjacent frames\n",
    "            for idx in drop_indices:\n",
    "                if idx > 0:\n",
    "                    frames[idx] = frames[idx - 1]\n",
    "                elif idx < n_frames - 1:\n",
    "                    frames[idx] = frames[idx + 1]\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(frames, prob=0.3, noise_factor=0.02):\n",
    "        \"\"\"Add gaussian noise to frames\"\"\"\n",
    "        if random.random() < prob:\n",
    "            noise = np.random.normal(0, noise_factor, frames.shape)\n",
    "            frames = frames + noise\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        return frames\n",
    "\n",
    "def preprocess_video(video_path, img_size=IMG_SIZE, frame_count=FRAME_COUNT, augment=False):\n",
    "    \"\"\"Enhanced video preprocessing with augmentation support\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Warning: Video not found: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames == 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    # Calculate frame step to uniformly sample frame_count frames\n",
    "    frame_step = max(1, total_frames // frame_count)\n",
    "    \n",
    "    for i in range(0, total_frames, frame_step):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize and normalize frame\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0  # Normalize to [0,1]\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # If fewer frames, pad with zeros or duplicate last frame\n",
    "    while len(frames) < frame_count:\n",
    "        if len(frames) > 0:\n",
    "            frames.append(frames[-1])  # Duplicate last frame\n",
    "        else:\n",
    "            frames.append(np.zeros((img_size[0], img_size[1], 3)))\n",
    "    \n",
    "    frames = np.array(frames[:frame_count])\n",
    "    \n",
    "    # Apply augmentations if training\n",
    "    if augment:\n",
    "        augmenter = VideoAugmentation()\n",
    "        frames = augmenter.random_horizontal_flip(frames)\n",
    "        frames = augmenter.random_brightness_adjustment(frames)\n",
    "        frames = augmenter.random_contrast_adjustment(frames)\n",
    "        frames = augmenter.random_rotation(frames)\n",
    "        frames = augmenter.random_temporal_crop(frames)\n",
    "        frames = augmenter.random_frame_dropout(frames)\n",
    "        frames = augmenter.add_gaussian_noise(frames)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def load_dataset(data_dir, classes=['Violence', 'NonViolence'], augment_training=True):\n",
    "    \"\"\"Load dataset with optional augmentation\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for label, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Warning: Directory not found: {class_dir}\")\n",
    "            continue\n",
    "            \n",
    "        video_files = [f for f in os.listdir(class_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        \n",
    "        for video_file in tqdm(video_files, desc=f\"Loading {class_name}\"):\n",
    "            video_path = os.path.join(class_dir, video_file)\n",
    "            frames = preprocess_video(video_path, augment=False)  # Base version\n",
    "            \n",
    "            if frames is not None:\n",
    "                X.append(frames)\n",
    "                y.append(label)\n",
    "                \n",
    "                # Add augmented versions for training data (especially for minority class)\n",
    "                if augment_training and (label == 0 or len(video_files) < 100):  # Violence class or small dataset\n",
    "                    for _ in range(2):  # Create 2 augmented versions\n",
    "                        aug_frames = preprocess_video(video_path, augment=True)\n",
    "                        if aug_frames is not None:\n",
    "                            X.append(aug_frames)\n",
    "                            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_model(input_shape=(FRAME_COUNT, IMG_SIZE[0], IMG_SIZE[1], 3)):\n",
    "    \"\"\"Enhanced 3D CNN model with batch normalization\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        \n",
    "        Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        \n",
    "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 0].set_title('Model Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 1].set_title('Model Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Precision\n",
    "    axes[1, 0].plot(history.history['precision'], label='Training Precision')\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Validation Precision')\n",
    "    axes[1, 0].set_title('Model Precision')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Recall\n",
    "    axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "    axes[1, 1].set_title('Model Recall')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=['NonViolence', 'Violence']):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model_comprehensive(model, X_test, y_test, classes=['NonViolence', 'Violence']):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_prob = model.predict(X_test).flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Basic metrics\n",
    "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall:    {recall:.4f}\")\n",
    "    print(f\"Test F1-Score:  {f1_score:.4f}\")\n",
    "    print(f\"Test AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Test Loss:      {loss:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(classification_report(y_test, y_pred, target_names=classes, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(\"-\" * 20)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]}\")\n",
    "    print(f\"False Positives: {cm[0,1]}\")\n",
    "    print(f\"False Negatives: {cm[1,0]}\")\n",
    "    print(f\"True Positives:  {cm[1,1]}\")\n",
    "    \n",
    "    # Plot visualizations\n",
    "    plot_confusion_matrix(y_test, y_pred, classes)\n",
    "    plot_roc_curve(y_test, y_prob)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'auc_score': auc_score,\n",
    "        'loss': loss\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    # Set dataset path\n",
    "    data_dir = r'C:\\Users\\Namo\\fight recognization\\extract\\Real Life Violence Dataset'\n",
    "    \n",
    "    print(\"Loading dataset with augmentation...\")\n",
    "    X, y = load_dataset(data_dir, augment_training=True)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"No data loaded. Please check your dataset path.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Violence samples: {np.sum(y)}\")\n",
    "    print(f\"Non-violence samples: {len(y) - np.sum(y)}\")\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    \n",
    "    # Calculate class weights for imbalanced dataset\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=np.unique(y_train), \n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_violence_detection_model.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    model.load_weights('best_violence_detection_model.keras')\n",
    "    metrics = evaluate_model_comprehensive(model, X_test, y_test)\n",
    "    \n",
    "    # Save final model\n",
    "    model.save('violence_detection_model_final.keras')\n",
    "    print(\"\\nModel saved as 'violence_detection_model_final.keras'\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open('model_metrics.txt', 'w') as f:\n",
    "        f.write(\"Violence Detection Model Metrics\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\")\n",
    "        for metric, value in metrics.items():\n",
    "            f.write(f\"{metric.capitalize()}: {value:.4f}\\n\")\n",
    "    \n",
    "    print(\"Training complete! Check the generated plots and metrics file.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00dd6c73-922f-42cb-a981-62d808cea2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa860ae-5f51-4113-bafa-8fb2622fbfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset with augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Violence:  20%|████████████▎                                                | 202/1000 [03:16<24:34,  1.85s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (64, 64)\n",
    "FRAME_COUNT = 16\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class VideoAugmentation:\n",
    "    \"\"\"Class for video data augmentation techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_horizontal_flip(frames, prob=0.5):\n",
    "        if random.random() < prob:\n",
    "            return np.flip(frames, axis=2)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_brightness_adjustment(frames, prob=0.5, factor_range=(0.7, 1.3)):\n",
    "        if random.random() < prob:\n",
    "            factor = random.uniform(*factor_range)\n",
    "            frames = frames * factor\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_contrast_adjustment(frames, prob=0.5, factor_range=(0.8, 1.2)):\n",
    "        if random.random() < prob:\n",
    "            factor = random.uniform(*factor_range)\n",
    "            mean = np.mean(frames)\n",
    "            frames = (frames - mean) * factor + mean\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_rotation(frames, prob=0.3, angle_range=(-10, 10)):\n",
    "        if random.random() < prob:\n",
    "            angle = random.uniform(*angle_range)\n",
    "            h, w = frames.shape[1:3]\n",
    "            center = (w // 2, h // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated_frames = []\n",
    "            for frame in frames:\n",
    "                rotated_frame = cv2.warpAffine(frame, rotation_matrix, (w, h))\n",
    "                rotated_frames.append(rotated_frame)\n",
    "            return np.array(rotated_frames)\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_temporal_crop(frames, prob=0.3):\n",
    "        if random.random() < prob and len(frames) > FRAME_COUNT // 2:\n",
    "            start_idx = random.randint(0, len(frames) - FRAME_COUNT // 2)\n",
    "            end_idx = start_idx + FRAME_COUNT // 2\n",
    "            cropped_frames = frames[start_idx:end_idx]\n",
    "            while len(cropped_frames) < FRAME_COUNT:\n",
    "                cropped_frames = np.concatenate([cropped_frames, cropped_frames])\n",
    "            return cropped_frames[:FRAME_COUNT]\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_frame_dropout(frames, prob=0.2, dropout_rate=0.1):\n",
    "        if random.random() < prob:\n",
    "            n_frames = len(frames)\n",
    "            n_drop = int(n_frames * dropout_rate)\n",
    "            drop_indices = random.sample(range(n_frames), n_drop)\n",
    "            for idx in drop_indices:\n",
    "                if idx > 0:\n",
    "                    frames[idx] = frames[idx - 1]\n",
    "                elif idx < n_frames - 1:\n",
    "                    frames[idx] = frames[idx + 1]\n",
    "        return frames\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(frames, prob=0.3, noise_factor=0.02):\n",
    "        if random.random() < prob:\n",
    "            noise = np.random.normal(0, noise_factor, frames.shape)\n",
    "            frames = frames + noise\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        return frames\n",
    "\n",
    "def preprocess_video(video_path, img_size=IMG_SIZE, frame_count=FRAME_COUNT, augment=False):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Warning: Video not found: {video_path}\")\n",
    "        return None\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video: {video_path}\")\n",
    "        return None\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames == 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "    frame_step = max(1, total_frames // frame_count)\n",
    "    for i in range(0, total_frames, frame_step):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    while len(frames) < frame_count:\n",
    "        if len(frames) > 0:\n",
    "            frames.append(frames[-1])\n",
    "        else:\n",
    "            frames.append(np.zeros((img_size[0], img_size[1], 3)))\n",
    "    frames = np.array(frames[:frame_count])\n",
    "    if augment:\n",
    "        augmenter = VideoAugmentation()\n",
    "        frames = augmenter.random_horizontal_flip(frames)\n",
    "        frames = augmenter.random_brightness_adjustment(frames)\n",
    "        frames = augmenter.random_contrast_adjustment(frames)\n",
    "        frames = augmenter.random_rotation(frames)\n",
    "        frames = augmenter.random_temporal_crop(frames)\n",
    "        frames = augmenter.random_frame_dropout(frames)\n",
    "        frames = augmenter.add_gaussian_noise(frames)\n",
    "    return frames\n",
    "\n",
    "def load_dataset(data_dir, classes=['Violence', 'NonViolence'], augment_training=True):\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Warning: Directory not found: {class_dir}\")\n",
    "            continue\n",
    "        video_files = [f for f in os.listdir(class_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        for video_file in tqdm(video_files, desc=f\"Loading {class_name}\"):\n",
    "            video_path = os.path.join(class_dir, video_file)\n",
    "            frames = preprocess_video(video_path, augment=False)\n",
    "            if frames is not None:\n",
    "                X.append(frames)\n",
    "                y.append(label)\n",
    "                if augment_training and (label == 0 or len(video_files) < 100):\n",
    "                    for _ in range(2):\n",
    "                        aug_frames = preprocess_video(video_path, augment=True)\n",
    "                        if aug_frames is not None:\n",
    "                            X.append(aug_frames)\n",
    "                            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_model(input_shape=(FRAME_COUNT, IMG_SIZE[0], IMG_SIZE[1], 3)):\n",
    "    model = Sequential([\n",
    "        Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def convert_to_tflite(model, output_path='violence_detection_model.tflite', quantization='float16'):\n",
    "    \"\"\"Convert Keras model to TFLite with optional quantization\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    if quantization == 'float16':\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    elif quantization == 'int8':\n",
    "        converter.inference_type = tf.int8\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        # Representative dataset for quantization (optional, improves accuracy)\n",
    "        def representative_dataset():\n",
    "            for _ in range(100):\n",
    "                yield [np.random.rand(1, FRAME_COUNT, IMG_SIZE[0], IMG_SIZE[1], 3).astype(np.float32)]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"TFLite model saved as '{output_path}'\")\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 0].set_title('Model Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 1].set_title('Model Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[1, 0].plot(history.history['precision'], label='Training Precision')\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Validation Precision')\n",
    "    axes[1, 0].set_title('Model Precision')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "    axes[1, 1].set_title('Model Recall')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=['NonViolence', 'Violence']):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model_comprehensive(model, X_test, y_test, classes=['NonViolence', 'Violence']):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    y_prob = model.predict(X_test).flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall:    {recall:.4f}\")\n",
    "    print(f\"Test F1-Score:  {f1_score:.4f}\")\n",
    "    print(f\"Test AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Test Loss:      {loss:.4f}\")\n",
    "    print(\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(classification_report(y_test, y_pred, target_names=classes, digits=4))\n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(\"-\" * 20)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]}\")\n",
    "    print(f\"False Positives: {cm[0,1]}\")\n",
    "    print(f\"False Negatives: {cm[1,0]}\")\n",
    "    print(f\"True Positives:  {cm[1,1]}\")\n",
    "    plot_confusion_matrix(y_test, y_pred, classes)\n",
    "    plot_roc_curve(y_test, y_prob)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'auc_score': auc_score,\n",
    "        'loss': loss\n",
    "    }\n",
    "\n",
    "def evaluate_tflite_model(tflite_path, X_test, y_test, classes=['NonViolence', 'Violence']):\n",
    "    \"\"\"Evaluate TFLite model\"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    y_prob = []\n",
    "    for sample in tqdm(X_test, desc=\"Evaluating TFLite model\"):\n",
    "        interpreter.set_tensor(input_details[0]['index'], sample[np.newaxis, ...].astype(np.float32))\n",
    "        interpreter.invoke()\n",
    "        prob = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        y_prob.append(prob)\n",
    "    \n",
    "    y_prob = np.array(y_prob).flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "    print(\"\\nTFLite Model Evaluation:\")\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=classes, digits=4))\n",
    "    \n",
    "    plot_confusion_matrix(y_test, y_pred, classes)\n",
    "    plot_roc_curve(y_test, y_prob)\n",
    "\n",
    "def main():\n",
    "    data_dir = r'C:\\Users\\Namo\\fight recognization\\extract\\Real Life Violence Dataset'\n",
    "    print(\"Loading dataset with augmentation...\")\n",
    "    X, y = load_dataset(data_dir, augment_training=True)\n",
    "    if len(X) == 0:\n",
    "        print(\"No data loaded. Please check your dataset path.\")\n",
    "        return\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Violence samples: {np.sum(y)}\")\n",
    "    print(f\"Non-violence samples: {len(y) - np.sum(y)}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_violence_detection_model.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    plot_training_history(history)\n",
    "    model.load_weights('best_violence_detection_model.keras')\n",
    "    metrics = evaluate_model_comprehensive(model, X_test, y_test)\n",
    "    print(\"\\nConverting to TFLite (Float16)...\")\n",
    "    convert_to_tflite(model, 'violence_detection_model_float16.tflite', quantization='float16')\n",
    "    print(\"\\nConverting to TFLite (Int8)...\")\n",
    "    convert_to_tflite(model, 'violence_detection_model_int8.tflite', quantization='int8')\n",
    "    print(\"\\nEvaluating TFLite Float16 model...\")\n",
    "    evaluate_tflite_model('violence_detection_model_float16.tflite', X_test, y_test)\n",
    "    with open('model_metrics.txt', 'w') as f:\n",
    "        f.write(\"Violence Detection Model Metrics\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\")\n",
    "        for metric, value in metrics.items():\n",
    "            f.write(f\"{metric.capitalize()}: {value:.4f}\\n\")\n",
    "    print(\"Training complete! Check the generated plots, metrics file, and TFLite models.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
